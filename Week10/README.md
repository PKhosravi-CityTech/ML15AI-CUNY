# ðŸ“Œ Week 10: NLP Fundamentals & Attention-Based Models

## ðŸŽ¯ Learning Objectives  
In this session, we will cover:

âœ… **Bridging Vision and Language** â€“ Combining CNNs and RNNs for sequential understanding (video âž” text analogy).  
âœ… **Building RNNs and GRUs in PyTorch** â€“ Step-by-step guide to modeling sequences.  
âœ… **Understanding Natural Language Challenges** â€“ Dealing with ambiguity, context, and data sparsity.  
âœ… **Word Embeddings** â€“ Implementing and comparing Word2Vec, GloVe, and random embeddings.  
âœ… **Sequence-to-Sequence (Seq2Seq) Models** â€“ Building translation models with attention mechanisms.  
âœ… **Visualizing Attention** â€“ Interpreting how models align source and target sequences during translation.

---

## ðŸ“‚ Open in Google Colab  
Click the button below to run the notebook interactively in **Google Colab**:

[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/PKhosravi-CityTech/ML15AI-CUNY/blob/main/Week10/Week10.ipynb)  

âœ… **Interactive coding â€” no installation needed.**  
âœ… **Build RNNs, GRUs, and Seq2Seq models directly in the notebook.**

---

## ðŸŽ¥ Additional Learning Resources  
Click the links below to **watch the YouTube lectures for Week 10**:

### ðŸ”¹ Natural Language Processing (NLP) Overview  
ðŸ“Œ **[Watch: What is NLP](https://youtu.be/fLvJ8VdHLA0?si=FjQJyKNet0PUJs92)**  

### ðŸ”¹ Recurrent Neural Networks (RNNs) and Variants  
ðŸ“Œ **[Watch: What is LSTM](https://youtu.be/b61DPVFX03I?si=SdAP_V-aih21lhB9)**  
ðŸ“Œ **[Watch: Illustrated Guide to LSTMs and GRUs](https://youtu.be/8HyCNIVRbSU?si=NBs6Ba0IyMfc11zL)**  

### ðŸ”¹ Word Embeddings  
ðŸ“Œ **[Watch: What are Word Embeddings?](https://youtu.be/wgfSDrqYMJ4?si=tifRhlmgEAbDMjVG)**  
ðŸ“Œ **[Watch: Word2Vec Explained](https://youtu.be/viZrOnJclY0?si=KKNAe4mAg4vZEju2)**  

### ðŸ”¹ Sequence-to-Sequence Models and Attention  
ðŸ“Œ **[Watch: Sequence-to-Sequence (Seq2Seq) Models](https://youtu.be/L8HKweZIOmg?si=MdxJc4V5866474a_)**  
ðŸ“Œ **[Watch: Attention for Neural Networks](https://youtu.be/PSs6nxngL6k?si=Vj2idUqys6nuOcyP)**  

---

## âœ… Next Steps  
ðŸ“Œ **Complete the notebook â€” build and train your first Seq2Seq model with attention!**  
ðŸ“Œ **Experiment with different embeddings (random, Word2Vec, GloVe).**  
ðŸ“Œ **Visualize attention weights to understand sequence alignment.**  
ðŸ“Œ **Try modifying the RNN and GRU architectures to improve performance.**  
ðŸ“Œ **Join the discussion in ML15AI-CUNY repo to share your results or ask questions.**

ðŸš€ Happy Learning! ðŸ˜ŠðŸ”¥

