# ðŸ“Œ Week 8: Introduction to Artificial Neural Networks

## ðŸŽ¯ Learning Objectives  
In this session, we will cover:

âœ… **Neural Networks vs Traditional ML Models** â€“ Understanding where neural networks shine.  
âœ… **Feedforward Neural Network Architecture** â€“ Exploring the layers, neurons, weights, and biases.  
âœ… **Activation Functions** â€“ Adding non-linearity with ReLU, Sigmoid, Tanh, and more.  
âœ… **Forward and Backward Propagation** â€“ Learning how neural networks learn.  
âœ… **Loss Functions & Optimization** â€“ Gradient descent, learning rate, and tuning.  
âœ… **Regularization Techniques** â€“ Preventing overfitting with dropout, L1/L2 regularization.  
âœ… **Hands-On with PyTorch** â€“ Building a simple neural network from scratch.  

---

## ðŸ“‚ Open in Google Colab  
Click the button below to run the notebook interactively in **Google Colab**:  

[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/PKhosravi-CityTech/ML15AI-CUNY/blob/main/Week8/Week8.ipynb)  

âœ… **Interactive coding â€” no installation needed.**  
âœ… **Code, train, and evaluate models directly in the notebook.**  

---

## ðŸŽ¥ Additional Learning Resources  
Click the links below to **watch the YouTube lectures for Week 8**:  

### ðŸ”¹ Machine Learning vs Deep Learning  
ðŸ“Œ **[Watch: ML vs DL](https://youtu.be/q6kJ71tEYqM?si=P6zRjRoGIDDPeUH3)**  

### ðŸ”¹ Neural Networks Introduction  
ðŸ“Œ **[Watch: Neural Networks](https://youtu.be/jmmW0F0biz0?si=PdtAp9Cq-13TCxpj)**  

### ðŸ”¹ The Perceptron  
ðŸ“Œ **[Watch: The Perceptron](https://youtu.be/i1G7PXZMnSc?si=5_ANaVbja_dCOsvE)**  

### ðŸ”¹ What are MLPs?  
ðŸ“Œ **[Watch: Multi-Layer Perceptrons](https://youtu.be/7YaqzpitBXw?si=hcPdojSWy-5Wz3UI)**  

### ðŸ”¹ Activation Functions  
ðŸ“Œ **[Watch: Activation Functions](https://youtu.be/56ZxEmGRt2k?si=Qvw2ZafFZIXhdCSf)**  

### ðŸ”¹ Vanishing/Exploding Gradients  
ðŸ“Œ **[Watch: Vanishing/Exploding Gradients](https://youtu.be/2f_45VzKEfE?si=KKBHag0GQFZRK9yR)**  

### ðŸ”¹ Gradient Descent  
ðŸ“Œ **[Watch: Gradient Descent](https://youtu.be/i62czvwDlsw?si=kQeRtfuJ-m3xG1-B)**  

### ðŸ”¹ Back Propagation  
ðŸ“Œ **[Watch: Backpropagation](https://youtu.be/S5AGN9XfPK4?si=vUdMiJnH2UXUxcQy)**  

### ðŸ”¹ Loss Functions  
ðŸ“Œ **[Watch: Loss Functions](https://youtu.be/v_ueBW_5dLg?si=O36nT40m-7vkcLi5)**  

### ðŸ”¹ Regularization  
ðŸ“Œ **[Watch: Regularization](https://youtu.be/EehRcPo1M-Q?si=zHoSlJQyDAiNjEvr)**  

### ðŸ”¹ TensorFlow Overview  
ðŸ“Œ **[Watch: TensorFlow](https://youtu.be/i8NETqtGHms?si=4lBwF2bmG6rxMWOA)**  

### ðŸ”¹ PyTorch Overview  
ðŸ“Œ **[Watch: PyTorch](https://youtu.be/ORMx45xqWkA?si=yJlYVgIhojIHWKvs)**  

### ðŸ”¹ PyTorch Crash Course  
ðŸ“Œ **[Watch: PyTorch Crash Course](https://youtu.be/OIenNRt2bjg?si=81MphB2rrTf5dqTZ)**  

### ðŸ”¹ Epochs, Iterations, and Batch Sizes  
ðŸ“Œ **[Watch: Epochs, Iterations & Batch](https://youtu.be/SftOqbMrGfE?si=CUHe5DFMHZJdPXwV)**  

---

## âœ… Next Steps  
ðŸ“Œ **Complete the notebook â€” build and train your own neural network!**  
ðŸ“Œ **Experiment with different activation functions and optimizers.**  
ðŸ“Œ **Play with hyperparameters like learning rate, batch size, and number of epochs.**  
ðŸ“Œ **Join the discussion in ML15AI-CUNY repo to share results or ask questions.**  

ðŸš€ **Happy Learning!** ðŸ˜ŠðŸ”¥


