{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://github.com/PKhosravi-CityTech/LightCnnRad/raw/main/Images/BioMindLogo.png\" alt=\"BioMind AI Lab Logo\" width=\"150\" height=\"150\" align=\"left\" style=\"margin-bottom: 40px;\"> **Repository Developed by Pegah Khosravi, Principal Investigator of the BioMind AI Lab**\n",
        "\n",
        "Welcome to this repository! This notebook is designed to provide hands-on experience and foundational knowledge in machine learning. It is part of our journey to explore key ML concepts, algorithms, and applications. Whether you're a PhD student, or a master's student, this repository aims to support your learning goals and encourage critical thinking about machine learning systems.\n"
      ],
      "metadata": {
        "id": "klVqqBYFKZ1_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quiz 1\n",
        "\n",
        "## Quiz Description\n",
        "\n",
        "You will have a maximum of 30 minutes to complete the quiz. However, if you are experiencing high stress or require additional time, you may take up to 1 hour to finish.\n",
        "\n",
        "## Quiz Structure\n",
        "- 6 Short-answer questions\n",
        "- 1 True/False question\n",
        "- 1 Fill-in-the-blank question (with two blanks)\n",
        "- 2 Essay questions\n",
        "\n",
        "## Instructions\n",
        "- For the short-answer questions, select one of the four options and write your answer as A, B, C, or D (capital letters preferred).\n",
        "- For the fill-in-the-blank question, provide one word per blank, separated by a comma (e.g., word1, word2).\n",
        "- For the essay questions, write your explanation and code in Google Colab or any Python-compatible platform.\n",
        "- This is an open-book quiz, so you may use textbooks or other resources—just make sure to work independently and do not use OpenAI or similar AI tools.\n",
        "\n",
        "Follow these instructions carefully. Good luck!"
      ],
      "metadata": {
        "id": "qtrQNHqLKr2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Markdown, clear_output\n",
        "\n",
        "# Function to create a question-answer toggle button\n",
        "def create_question(question_text, answer_text):\n",
        "    button = widgets.Button(description=\"Click to Reveal Answer\")\n",
        "    output = widgets.Output()\n",
        "\n",
        "    def reveal_answer(b):\n",
        "        with output:\n",
        "            output.clear_output()\n",
        "            display(Markdown(f\"**Answer:** {answer_text}\"))\n",
        "\n",
        "    button.on_click(reveal_answer)\n",
        "    display(Markdown(f\"**{question_text}**\"), button, output)\n",
        "\n",
        "# List of questions and answers\n",
        "qa_pairs = [\n",
        "    (\"Question 1: What is the key difference between generative and discriminative models?\\n\"\n",
        "     \"A) Generative models estimate P(Y∣X), while discriminative models estimate P(X∣Y)\\n\"\n",
        "     \"B) Discriminative models estimate P(Y∣X), while generative models estimate P(X,Y)\\n\"\n",
        "     \"C) Generative models only work with supervised learning, while discriminative models work with unsupervised learning\\n\"\n",
        "     \"D) Discriminative models learn the joint probability distribution of the data\", \"B\"),\n",
        "\n",
        "    (\"Question 2: Which of the following regularization techniques helps prevent overfitting by adding the sum of squared weights to the loss function?\\n\"\n",
        "     \"A) L1 regularization\\nB) Dropout\\nC) L2 regularization\\nD) Batch Normalization\", \"C\"),\n",
        "\n",
        "    (\"Question 3: What is the main purpose of the bias-variance tradeoff in machine learning?\\n\"\n",
        "     \"A) To increase the accuracy of a model\\nB) To reduce both bias and variance simultaneously\\n\"\n",
        "     \"C) To find a balance between underfitting and overfitting\\nD) To determine the optimal number of decision trees in an ensemble\", \"C\"),\n",
        "\n",
        "    (\"Question 4: Which of the following statements about decision trees is FALSE?\\n\"\n",
        "     \"A) Decision trees can handle both numerical and categorical data\\n\"\n",
        "     \"B) Decision trees are prone to overfitting, especially with deep trees\\n\"\n",
        "     \"C) Decision trees use impurity measures such as Gini impurity and entropy\\n\"\n",
        "     \"D) Decision trees require feature scaling before training\", \"D\"),\n",
        "\n",
        "    (\"Question 5: What is the main advantage of ensemble methods like Random Forest over a single decision tree?\\n\"\n",
        "     \"A) They reduce variance and improve generalization\\n\"\n",
        "     \"B) They are always more interpretable than a single decision tree\\n\"\n",
        "     \"C) They require less computational power than decision trees\\n\"\n",
        "     \"D) They eliminate the need for feature selection\", \"A\"),\n",
        "\n",
        "    (\"Question 6: Which boosting algorithm works by iteratively training models that correct the errors of previous models?\\n\"\n",
        "     \"A) Bagging\\nB) AdaBoost\\nC) K-Means\\nD) Principal Component Analysis (PCA)\", \"B\"),\n",
        "\n",
        "    (\"Question 7: True/False? Increasing the complexity of a model always leads to better performance on unseen data.\", \"False\"),\n",
        "\n",
        "    (\"Question 8: Fill in the Blank -> Logistic Regression is commonly used for ______ problems where the target variable has two possible outcomes. \"\n",
        "     \"Unlike linear regression, it applies the ______ function to map predictions to probability values between 0 and 1.\", \"classification, sigmoid\"),\n",
        "\n",
        "]\n",
        "\n",
        "# Display all questions with interactive answer buttons\n",
        "for question, answer in qa_pairs:\n",
        "    create_question(question, answer)\n"
      ],
      "metadata": {
        "id": "Auq2jYv3MwLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Markdown, clear_output\n",
        "\n",
        "# Function to create a question-answer toggle button\n",
        "def create_question(question_text, answer_text):\n",
        "    button = widgets.Button(description=\"Click to Reveal Answer\")\n",
        "    output = widgets.Output()\n",
        "\n",
        "    def reveal_answer(b):\n",
        "        with output:\n",
        "            output.clear_output()\n",
        "            display(Markdown(f\"**Answer:**\\n\\n{answer_text}\"))\n",
        "\n",
        "    button.on_click(reveal_answer)\n",
        "    display(Markdown(question_text), button, output)\n",
        "\n",
        "# Corrected question formatting for proper Markdown rendering\n",
        "question_9 = (\n",
        "    \"**Question 9: Essay - Compare and Contrast Random Forest and XGBoost**\\n\\n\"\n",
        "    \"- **Explain the key differences** between Random Forest and XGBoost in terms of their learning strategies.\\n\"\n",
        "    \"- **How does each method handle decision trees** and optimize performance?\\n\"\n",
        "    \"- **When would you prefer** to use Random Forest over XGBoost and vice versa?\\n\"\n",
        ")\n",
        "\n",
        "answer_9 = (\n",
        "    \"**Random Forest and XGBoost** are both ensemble learning algorithms that use decision trees as their base models, \"\n",
        "    \"but they differ in how they build and optimize these trees.\\n\\n\"\n",
        "    \"**Random Forest** is based on **bagging** (Bootstrap Aggregating), where multiple decision trees are trained independently \"\n",
        "    \"on different random subsets of the data. The final prediction is made by averaging (for regression) or majority voting \"\n",
        "    \"(for classification) across all trees. Each tree in a Random Forest is grown fully without pruning, making it robust \"\n",
        "    \"but also computationally expensive.\\n\\n\"\n",
        "    \"**XGBoost (Extreme Gradient Boosting)**, on the other hand, uses **boosting**, meaning trees are built sequentially, \"\n",
        "    \"with each new tree correcting the mistakes of the previous ones. Instead of training trees independently, XGBoost applies \"\n",
        "    \"gradient boosting, where each new tree learns to minimize the residual errors of the previous trees. XGBoost also \"\n",
        "    \"incorporates **regularization techniques (L1 and L2 penalties)** to reduce overfitting and supports parallel computing, \"\n",
        "    \"making it much faster and more efficient for large datasets compared to Random Forest.\\n\\n\"\n",
        "    \"**When to Use Which?**\\n\\n\"\n",
        "    \"- **Random Forest** is better for simple, interpretable models and high-variance datasets. It handles complex relationships well but can be computationally expensive.\\n\"\n",
        "    \"- **XGBoost** is preferred when speed and performance are critical, especially in large-scale applications and competitions. It controls overfitting better due to built-in regularization.\\n\\n\"\n",
        "    \"In summary, **Random Forest is easier to implement and interpret, while XGBoost is more powerful for complex problems** requiring high accuracy. \"\n",
        "    \"However, its sequential boosting approach makes it computationally more expensive than Random Forest, though optimizations make it faster than traditional gradient boosting methods.\"\n",
        ")\n",
        "\n",
        "# Display the question\n",
        "create_question(question_9, answer_9)\n"
      ],
      "metadata": {
        "id": "ec8bQ0hRP3MI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Markdown, clear_output\n",
        "\n",
        "def create_question(question_text, answer_text):\n",
        "    button = widgets.Button(description=\"Click to Reveal Answer\")\n",
        "    output = widgets.Output()\n",
        "\n",
        "    def reveal_answer(b):\n",
        "        with output:\n",
        "            output.clear_output()\n",
        "            display(Markdown(f\"**Answer:**\\n\\n{answer_text}\"))\n",
        "\n",
        "    button.on_click(reveal_answer)\n",
        "    display(Markdown(question_text), button, output)\n",
        "\n",
        "question_10 = (\n",
        "    \"**Question 10: Essay & Code - Housing Price Classification**\\n\\n\"\n",
        "    \"You are provided with a housing dataset containing the following features:\\n\"\n",
        "    \"- **RM**: Average number of rooms per dwelling.\\n\"\n",
        "    \"- **LSTAT**: Percentage of lower-status population in the area.\\n\"\n",
        "    \"- **PTRATIO**: Pupil-teacher ratio in the neighborhood.\\n\"\n",
        "    \"- **MEDV**: House price in dollars (target variable).\\n\\n\"\n",
        "    \"**Task:**\\n\"\n",
        "    \"- Convert the `MEDV` column into a binary classification target:\\n\"\n",
        "      \"  - Calculate the **median** house price.\\n\"\n",
        "      \"  - If `MEDV` is above the median, classify it as `'High' (1)`.\\n\"\n",
        "      \"  - If `MEDV` is below or equal to the median, classify it as `'Low' (0)`.\\n\"\n",
        "    \"- Split the data into training (70%) and testing (30%) sets.\\n\"\n",
        "    \"- Train a **Decision Tree Classifier** with:\\n\"\n",
        "      \"  - Features: `RM`, `LSTAT`, `PTRATIO`.\\n\"\n",
        "      \"  - Max Depth: `3`.\\n\"\n",
        "      \"  - Random State: `42`.\\n\"\n",
        "    \"- Evaluate the model:\\n\"\n",
        "      \"  - Compute accuracy.\\n\"\n",
        "      \"  - Display the confusion matrix.\\n\"\n",
        "      \"  - Generate the classification report.\\n\"\n",
        "    \"- **Visualize** the Decision Tree structure.\\n\\n\"\n",
        "    \"**Interpretation Questions:**\\n\"\n",
        "    \"- Which feature appears at the top of the tree?\\n\"\n",
        "    \"- What does this tell you about its importance?\\n\"\n",
        "    \"- Suggest one improvement to this model.\"\n",
        ")\n",
        "\n",
        "answer_10 = (\n",
        "    \"**Python Implementation**\\n\\n\"\n",
        "    \"```python\\n\"\n",
        "    \"import numpy as np\\n\"\n",
        "    \"import pandas as pd\\n\"\n",
        "    \"import matplotlib.pyplot as plt\\n\"\n",
        "    \"from sklearn.model_selection import train_test_split\\n\"\n",
        "    \"from sklearn.tree import DecisionTreeClassifier, plot_tree\\n\"\n",
        "    \"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\\n\\n\"\n",
        "    \"df = pd.read_csv('/content/housing.csv')\\n\"\n",
        "    \"median_price = df['MEDV'].median()\\n\"\n",
        "    \"df['PriceCategory'] = (df['MEDV'] > median_price).astype(int)\\n\"\n",
        "    \"y = df['PriceCategory']\\n\"\n",
        "    \"X = df[['RM', 'LSTAT', 'PTRATIO']]\\n\"\n",
        "    \"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\\n\\n\"\n",
        "    \"clf = DecisionTreeClassifier(max_depth=3, random_state=42)\\n\"\n",
        "    \"clf.fit(X_train, y_train)\\n\"\n",
        "    \"y_pred = clf.predict(X_test)\\n\\n\"\n",
        "    \"print(f'Accuracy: {accuracy_score(y_test, y_pred):.4f}')\\n\"\n",
        "    \"print('\\\\nConfusion Matrix:\\\\n', confusion_matrix(y_test, y_pred))\\n\"\n",
        "    \"print('\\\\nClassification Report:\\\\n', classification_report(y_test, y_pred))\\n\\n\"\n",
        "    \"plt.figure(figsize=(15, 8))\\n\"\n",
        "    \"plot_tree(clf, feature_names=X.columns, class_names=['Low', 'High'], filled=True)\\n\"\n",
        "    \"plt.show()\\n\"\n",
        "    \"```\\n\\n\"\n",
        "    \"**Interpreting the Results**\\n\\n\"\n",
        "    \"- **Which feature appears at the top?**\\n\"\n",
        "    \"  - LSTAT (Percentage of lower-status population in the area).\\n\\n\"\n",
        "    \"- **What does this indicate?**\\n\"\n",
        "    \"  - The top feature is the most important in classification. Since LSTAT appears first, it suggests that \"\n",
        "    \"areas with a lower percentage of lower-status residents are more likely to have higher house prices.\\n\\n\"\n",
        "    \"- **Suggested improvements:**\\n\"\n",
        "    \"  - Tune hyperparameters (increase max depth, adjust min samples per leaf).\\n\"\n",
        "    \"  - Use **Random Forest or XGBoost** for better generalization.\\n\"\n",
        "    \"  - Include additional features like crime rate, highway accessibility, or property tax rate.\"\n",
        ")\n",
        "\n",
        "create_question(question_10, answer_10)\n"
      ],
      "metadata": {
        "id": "vJR5Y-EaSHAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 📥 Download `housing.csv` from Kaggle and Upload to Colab  \n",
        "#### 🔗 Dataset Link: [Boston Housing Dataset](https://www.kaggle.com/datasets/schirmerchad/bostonhoustingmlnd)  \n"
      ],
      "metadata": {
        "id": "ZSzr1RCWYeJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np  # For numerical operations\n",
        "import pandas as pd  # For handling data\n",
        "import matplotlib.pyplot as plt  # For visualization\n",
        "from sklearn.model_selection import train_test_split  # For splitting data\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree  # For decision tree classification and visualization\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report  # For model evaluation\n",
        "\n",
        "# Load the housing dataset\n",
        "df = pd.read_csv('/content/housing.csv')  # Read the dataset into a Pandas DataFrame\n",
        "\n",
        "# Convert the 'MEDV' column into a binary classification target\n",
        "median_price = df['MEDV'].median()  # Calculate the median house price\n",
        "df['PriceCategory'] = (df['MEDV'] > median_price).astype(int)  # Convert to binary (1 if above median, 0 otherwise)\n",
        "\n",
        "# Define the target variable (y) and features (X)\n",
        "y = df['PriceCategory']  # Target variable: PriceCategory (High = 1, Low = 0)\n",
        "X = df[['RM', 'LSTAT', 'PTRATIO']]  # Select features: RM (rooms), LSTAT (low-income %), PTRATIO (pupil-teacher ratio)\n",
        "\n",
        "# Split the dataset into training (70%) and testing (30%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "# 'random_state=42' ensures results are reproducible\n",
        "\n",
        "# Train a Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(max_depth=3, random_state=42)  # Create a decision tree model with max depth = 3\n",
        "clf.fit(X_train, y_train)  # Train the model using the training dataset\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)  # Predict housing price category (High/Low) for test data\n",
        "\n",
        "# Evaluate the model's performance\n",
        "print(f'Accuracy: {accuracy_score(y_test, y_pred):.4f}')  # Print the accuracy of the model\n",
        "print('\\nConfusion Matrix:\\n', confusion_matrix(y_test, y_pred))  # Show confusion matrix (TP, FP, TN, FN)\n",
        "print('\\nClassification Report:\\n', classification_report(y_test, y_pred))  # Show precision, recall, and F1-score\n",
        "\n",
        "# Visualize the trained Decision Tree model\n",
        "plt.figure(figsize=(15, 8))  # Set figure size for better readability\n",
        "plot_tree(clf, feature_names=X.columns, class_names=['Low', 'High'], filled=True)  # Plot the decision tree\n",
        "plt.show()  # Display the visualization\n"
      ],
      "metadata": {
        "id": "fNg-Of3tVPPI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}