{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://github.com/PKhosravi-CityTech/LightCnnRad/raw/main/Images/BioMindLogo.png\" alt=\"BioMind AI Lab Logo\" width=\"150\" height=\"150\" align=\"left\" style=\"margin-bottom: 40px;\"> **Repository Developed by Pegah Khosravi, Principal Investigator of the BioMind AI Lab**\n",
        "\n",
        "Welcome to this repository! This notebook is designed to provide hands-on experience and foundational knowledge in machine learning. It is part of our journey to explore key ML concepts, algorithms, and applications. Whether you're a PhD student, or a master's student, this repository aims to support your learning goals and encourage critical thinking about machine learning systems.\n"
      ],
      "metadata": {
        "id": "klVqqBYFKZ1_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quiz 1\n",
        "\n",
        "## Quiz Description\n",
        "\n",
        "You will have a maximum of 30 minutes to complete the quiz. However, if you are experiencing high stress or require additional time, you may take up to 1 hour to finish.\n",
        "\n",
        "## Quiz Structure\n",
        "- 6 Short-answer questions\n",
        "- 1 True/False question\n",
        "- 1 Fill-in-the-blank question (with two blanks)\n",
        "- 2 Essay questions\n",
        "\n",
        "## Instructions\n",
        "- For the short-answer questions, select one of the four options and write your answer as A, B, C, or D (capital letters preferred).\n",
        "- For the fill-in-the-blank question, provide one word per blank, separated by a comma (e.g., word1, word2).\n",
        "- For the essay questions, write your explanation and code in Google Colab or any Python-compatible platform.\n",
        "- This is an open-book quiz, so you may use textbooks or other resourcesâ€”just make sure to work independently and do not use OpenAI or similar AI tools.\n",
        "\n",
        "Follow these instructions carefully. Good luck!"
      ],
      "metadata": {
        "id": "qtrQNHqLKr2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Markdown, clear_output\n",
        "\n",
        "# Function to create a question-answer toggle button\n",
        "def create_question(question_text, answer_text):\n",
        "    button = widgets.Button(description=\"Click to Reveal Answer\")\n",
        "    output = widgets.Output()\n",
        "\n",
        "    def reveal_answer(b):\n",
        "        with output:\n",
        "            output.clear_output()\n",
        "            display(Markdown(f\"**Answer:** {answer_text}\"))\n",
        "\n",
        "    button.on_click(reveal_answer)\n",
        "    display(Markdown(f\"**{question_text}**\"), button, output)\n",
        "\n",
        "# List of questions and answers\n",
        "qa_pairs = [\n",
        "    (\"Question 1: What is the key difference between generative and discriminative models?\\n\"\n",
        "     \"A) Generative models estimate P(Yâˆ£X), while discriminative models estimate P(Xâˆ£Y)\\n\"\n",
        "     \"B) Discriminative models estimate P(Yâˆ£X), while generative models estimate P(X,Y)\\n\"\n",
        "     \"C) Generative models only work with supervised learning, while discriminative models work with unsupervised learning\\n\"\n",
        "     \"D) Discriminative models learn the joint probability distribution of the data\", \"B\"),\n",
        "\n",
        "    (\"Question 2: Which of the following regularization techniques helps prevent overfitting by adding the sum of squared weights to the loss function?\\n\"\n",
        "     \"A) L1 regularization\\nB) Dropout\\nC) L2 regularization\\nD) Batch Normalization\", \"C\"),\n",
        "\n",
        "    (\"Question 3: What is the main purpose of the bias-variance tradeoff in machine learning?\\n\"\n",
        "     \"A) To increase the accuracy of a model\\nB) To reduce both bias and variance simultaneously\\n\"\n",
        "     \"C) To find a balance between underfitting and overfitting\\nD) To determine the optimal number of decision trees in an ensemble\", \"C\"),\n",
        "\n",
        "    (\"Question 4: Which of the following statements about decision trees is FALSE?\\n\"\n",
        "     \"A) Decision trees can handle both numerical and categorical data\\n\"\n",
        "     \"B) Decision trees are prone to overfitting, especially with deep trees\\n\"\n",
        "     \"C) Decision trees use impurity measures such as Gini impurity and entropy\\n\"\n",
        "     \"D) Decision trees require feature scaling before training\", \"D\"),\n",
        "\n",
        "    (\"Question 5: What is the main advantage of ensemble methods like Random Forest over a single decision tree?\\n\"\n",
        "     \"A) They reduce variance and improve generalization\\n\"\n",
        "     \"B) They are always more interpretable than a single decision tree\\n\"\n",
        "     \"C) They require less computational power than decision trees\\n\"\n",
        "     \"D) They eliminate the need for feature selection\", \"A\"),\n",
        "\n",
        "    (\"Question 6: Which boosting algorithm works by iteratively training models that correct the errors of previous models?\\n\"\n",
        "     \"A) Bagging\\nB) AdaBoost\\nC) K-Means\\nD) Principal Component Analysis (PCA)\", \"B\"),\n",
        "\n",
        "    (\"Question 7: True/False? Increasing the complexity of a model always leads to better performance on unseen data.\", \"False\"),\n",
        "\n",
        "    (\"Question 8: Fill in the Blank -> Logistic Regression is commonly used for ______ problems where the target variable has two possible outcomes. \"\n",
        "     \"Unlike linear regression, it applies the ______ function to map predictions to probability values between 0 and 1.\", \"classification, sigmoid\"),\n",
        "\n",
        "]\n",
        "\n",
        "# Display all questions with interactive answer buttons\n",
        "for question, answer in qa_pairs:\n",
        "    create_question(question, answer)\n"
      ],
      "metadata": {
        "id": "Auq2jYv3MwLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Markdown, clear_output\n",
        "\n",
        "# Function to create a question-answer toggle button\n",
        "def create_question(question_text, answer_text):\n",
        "    button = widgets.Button(description=\"Click to Reveal Answer\")\n",
        "    output = widgets.Output()\n",
        "\n",
        "    def reveal_answer(b):\n",
        "        with output:\n",
        "            output.clear_output()\n",
        "            display(Markdown(f\"**Answer:**\\n\\n{answer_text}\"))\n",
        "\n",
        "    button.on_click(reveal_answer)\n",
        "    display(Markdown(question_text), button, output)\n",
        "\n",
        "# Corrected question formatting for proper Markdown rendering\n",
        "question_9 = (\n",
        "    \"**Question 9: Essay - Compare and Contrast Random Forest and XGBoost**\\n\\n\"\n",
        "    \"- **Explain the key differences** between Random Forest and XGBoost in terms of their learning strategies.\\n\"\n",
        "    \"- **How does each method handle decision trees** and optimize performance?\\n\"\n",
        "    \"- **When would you prefer** to use Random Forest over XGBoost and vice versa?\\n\"\n",
        ")\n",
        "\n",
        "answer_9 = (\n",
        "    \"**Random Forest and XGBoost** are both ensemble learning algorithms that use decision trees as their base models, \"\n",
        "    \"but they differ in how they build and optimize these trees.\\n\\n\"\n",
        "    \"**Random Forest** is based on **bagging** (Bootstrap Aggregating), where multiple decision trees are trained independently \"\n",
        "    \"on different random subsets of the data. The final prediction is made by averaging (for regression) or majority voting \"\n",
        "    \"(for classification) across all trees. Each tree in a Random Forest is grown fully without pruning, making it robust \"\n",
        "    \"but also computationally expensive.\\n\\n\"\n",
        "    \"**XGBoost (Extreme Gradient Boosting)**, on the other hand, uses **boosting**, meaning trees are built sequentially, \"\n",
        "    \"with each new tree correcting the mistakes of the previous ones. Instead of training trees independently, XGBoost applies \"\n",
        "    \"gradient boosting, where each new tree learns to minimize the residual errors of the previous trees. XGBoost also \"\n",
        "    \"incorporates **regularization techniques (L1 and L2 penalties)** to reduce overfitting and supports parallel computing, \"\n",
        "    \"making it much faster and more efficient for large datasets compared to Random Forest.\\n\\n\"\n",
        "    \"**When to Use Which?**\\n\\n\"\n",
        "    \"- **Random Forest** is better for simple, interpretable models and high-variance datasets. It handles complex relationships well but can be computationally expensive.\\n\"\n",
        "    \"- **XGBoost** is preferred when speed and performance are critical, especially in large-scale applications and competitions. It controls overfitting better due to built-in regularization.\\n\\n\"\n",
        "    \"In summary, **Random Forest is easier to implement and interpret, while XGBoost is more powerful for complex problems** requiring high accuracy. \"\n",
        "    \"However, its sequential boosting approach makes it computationally more expensive than Random Forest, though optimizations make it faster than traditional gradient boosting methods.\"\n",
        ")\n",
        "\n",
        "# Display the question\n",
        "create_question(question_9, answer_9)\n"
      ],
      "metadata": {
        "id": "ec8bQ0hRP3MI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Markdown, clear_output\n",
        "\n",
        "def create_question(question_text, answer_text):\n",
        "    button = widgets.Button(description=\"Click to Reveal Answer\")\n",
        "    output = widgets.Output()\n",
        "\n",
        "    def reveal_answer(b):\n",
        "        with output:\n",
        "            output.clear_output()\n",
        "            display(Markdown(f\"**Answer:**\\n\\n{answer_text}\"))\n",
        "\n",
        "    button.on_click(reveal_answer)\n",
        "    display(Markdown(question_text), button, output)\n",
        "\n",
        "question_10 = (\n",
        "    \"**Question 10: Essay & Code - Housing Price Classification**\\n\\n\"\n",
        "    \"You are provided with a housing dataset containing the following features:\\n\"\n",
        "    \"- **RM**: Average number of rooms per dwelling.\\n\"\n",
        "    \"- **LSTAT**: Percentage of lower-status population in the area.\\n\"\n",
        "    \"- **PTRATIO**: Pupil-teacher ratio in the neighborhood.\\n\"\n",
        "    \"- **MEDV**: House price in dollars (target variable).\\n\\n\"\n",
        "    \"**Task:**\\n\"\n",
        "    \"- Convert the `MEDV` column into a binary classification target:\\n\"\n",
        "      \"  - Calculate the **median** house price.\\n\"\n",
        "      \"  - If `MEDV` is above the median, classify it as `'High' (1)`.\\n\"\n",
        "      \"  - If `MEDV` is below or equal to the median, classify it as `'Low' (0)`.\\n\"\n",
        "    \"- Split the data into training (70%) and testing (30%) sets.\\n\"\n",
        "    \"- Train a **Decision Tree Classifier** with:\\n\"\n",
        "      \"  - Features: `RM`, `LSTAT`, `PTRATIO`.\\n\"\n",
        "      \"  - Max Depth: `3`.\\n\"\n",
        "      \"  - Random State: `42`.\\n\"\n",
        "    \"- Evaluate the model:\\n\"\n",
        "      \"  - Compute accuracy.\\n\"\n",
        "      \"  - Display the confusion matrix.\\n\"\n",
        "      \"  - Generate the classification report.\\n\"\n",
        "    \"- **Visualize** the Decision Tree structure.\\n\\n\"\n",
        "    \"**Interpretation Questions:**\\n\"\n",
        "    \"- Which feature appears at the top of the tree?\\n\"\n",
        "    \"- What does this tell you about its importance?\\n\"\n",
        "    \"- Suggest one improvement to this model.\"\n",
        ")\n",
        "\n",
        "answer_10 = (\n",
        "    \"**Python Implementation**\\n\\n\"\n",
        "    \"```python\\n\"\n",
        "    \"import numpy as np\\n\"\n",
        "    \"import pandas as pd\\n\"\n",
        "    \"import matplotlib.pyplot as plt\\n\"\n",
        "    \"from sklearn.model_selection import train_test_split\\n\"\n",
        "    \"from sklearn.tree import DecisionTreeClassifier, plot_tree\\n\"\n",
        "    \"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\\n\\n\"\n",
        "    \"df = pd.read_csv('/content/housing.csv')\\n\"\n",
        "    \"median_price = df['MEDV'].median()\\n\"\n",
        "    \"df['PriceCategory'] = (df['MEDV'] > median_price).astype(int)\\n\"\n",
        "    \"y = df['PriceCategory']\\n\"\n",
        "    \"X = df[['RM', 'LSTAT', 'PTRATIO']]\\n\"\n",
        "    \"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\\n\\n\"\n",
        "    \"clf = DecisionTreeClassifier(max_depth=3, random_state=42)\\n\"\n",
        "    \"clf.fit(X_train, y_train)\\n\"\n",
        "    \"y_pred = clf.predict(X_test)\\n\\n\"\n",
        "    \"print(f'Accuracy: {accuracy_score(y_test, y_pred):.4f}')\\n\"\n",
        "    \"print('\\\\nConfusion Matrix:\\\\n', confusion_matrix(y_test, y_pred))\\n\"\n",
        "    \"print('\\\\nClassification Report:\\\\n', classification_report(y_test, y_pred))\\n\\n\"\n",
        "    \"plt.figure(figsize=(15, 8))\\n\"\n",
        "    \"plot_tree(clf, feature_names=X.columns, class_names=['Low', 'High'], filled=True)\\n\"\n",
        "    \"plt.show()\\n\"\n",
        "    \"```\\n\\n\"\n",
        "    \"**Interpreting the Results**\\n\\n\"\n",
        "    \"- **Which feature appears at the top?**\\n\"\n",
        "    \"  - LSTAT (Percentage of lower-status population in the area).\\n\\n\"\n",
        "    \"- **What does this indicate?**\\n\"\n",
        "    \"  - The top feature is the most important in classification. Since LSTAT appears first, it suggests that \"\n",
        "    \"areas with a lower percentage of lower-status residents are more likely to have higher house prices.\\n\\n\"\n",
        "    \"- **Suggested improvements:**\\n\"\n",
        "    \"  - Tune hyperparameters (increase max depth, adjust min samples per leaf).\\n\"\n",
        "    \"  - Use **Random Forest or XGBoost** for better generalization.\\n\"\n",
        "    \"  - Include additional features like crime rate, highway accessibility, or property tax rate.\"\n",
        ")\n",
        "\n",
        "create_question(question_10, answer_10)\n"
      ],
      "metadata": {
        "id": "vJR5Y-EaSHAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ“¥ Download `housing.csv` from Kaggle and Upload to Colab  \n",
        "#### ðŸ”— Dataset Link: [Boston Housing Dataset](https://www.kaggle.com/datasets/schirmerchad/bostonhoustingmlnd)  \n"
      ],
      "metadata": {
        "id": "ZSzr1RCWYeJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np  # For numerical operations\n",
        "import pandas as pd  # For handling data\n",
        "import matplotlib.pyplot as plt  # For visualization\n",
        "from sklearn.model_selection import train_test_split  # For splitting data\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree  # For decision tree classification and visualization\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report  # For model evaluation\n",
        "\n",
        "# Load the housing dataset\n",
        "df = pd.read_csv('/content/housing.csv')  # Read the dataset into a Pandas DataFrame\n",
        "\n",
        "# Convert the 'MEDV' column into a binary classification target\n",
        "median_price = df['MEDV'].median()  # Calculate the median house price\n",
        "df['PriceCategory'] = (df['MEDV'] > median_price).astype(int)  # Convert to binary (1 if above median, 0 otherwise)\n",
        "\n",
        "# Define the target variable (y) and features (X)\n",
        "y = df['PriceCategory']  # Target variable: PriceCategory (High = 1, Low = 0)\n",
        "X = df[['RM', 'LSTAT', 'PTRATIO']]  # Select features: RM (rooms), LSTAT (low-income %), PTRATIO (pupil-teacher ratio)\n",
        "\n",
        "# Split the dataset into training (70%) and testing (30%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "# 'random_state=42' ensures results are reproducible\n",
        "\n",
        "# Train a Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(max_depth=3, random_state=42)  # Create a decision tree model with max depth = 3\n",
        "clf.fit(X_train, y_train)  # Train the model using the training dataset\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)  # Predict housing price category (High/Low) for test data\n",
        "\n",
        "# Evaluate the model's performance\n",
        "print(f'Accuracy: {accuracy_score(y_test, y_pred):.4f}')  # Print the accuracy of the model\n",
        "print('\\nConfusion Matrix:\\n', confusion_matrix(y_test, y_pred))  # Show confusion matrix (TP, FP, TN, FN)\n",
        "print('\\nClassification Report:\\n', classification_report(y_test, y_pred))  # Show precision, recall, and F1-score\n",
        "\n",
        "# Visualize the trained Decision Tree model\n",
        "plt.figure(figsize=(15, 8))  # Set figure size for better readability\n",
        "plot_tree(clf, feature_names=X.columns, class_names=['Low', 'High'], filled=True)  # Plot the decision tree\n",
        "plt.show()  # Display the visualization\n"
      ],
      "metadata": {
        "id": "fNg-Of3tVPPI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}